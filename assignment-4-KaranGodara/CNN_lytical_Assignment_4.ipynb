{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-lytical Assignment-4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-lytical Assignment-4\n",
        "<center>\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7fZ0PJ4leQi4qtXR5Egv5YILqQqvzVSNtFg&usqp=CAU\">\n",
        "</center>\n",
        "\n",
        "*  In this assignment, we will use CNNs in [PyTorch](https://pytorch.org/docs/stable/index.html) for image segmentation.\n",
        "\n",
        "* Segmentation is the task of classifying pixels into classes - whether those pixels are a part of a certain object or not.\n",
        "\n",
        "* The model we will be using is UNet, you can read about it here - https://arxiv.org/pdf/1505.04597.pdf\n",
        "\n",
        "* Much of what you learnt in Assignments 2 and 3 will carry over to this assignment. The structure would overall remain the same - a Dataset class, a Model class and a Training loop. However, significant effort would go into creating these because UNet is a complex model in itself.\n",
        "\n",
        "* **No processing of the data should happen outside the Dataset class, which should take in raw inputs directly from the unpickled object**.\n",
        "\n",
        "* Because this would be the first time you deal with a complex model, feel free to refer to this video - https://www.youtube.com/watch?v=u1loyDCoGbE - Abhishek Thakur (one of the top Kaggle GMs) walks you through the design of the model and how you can make subclasses to make your code clean and reusable.\n",
        "\n",
        "**Feel free to redefine any function signatures below, just make sure the final cell remains the same.**"
      ],
      "metadata": {
        "id": "Xwo8D8V-uPsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries here\n",
        "PyTorch, NumPy, Matplotlib, ...\n",
        "Even when equipped with PyTorch, NumPy and Matplotlib make your work easier for visualization etc.\n",
        "\n",
        "Note the following method to **initialize the seed** for reproducibility of results, both for NumPy & PyTorch (CPU/CUDA)."
      ],
      "metadata": {
        "id": "QQNvtQCE_j1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnnyxVTxqpZB"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load *Dataset*\n",
        "Use the [pickle file](https://drive.google.com/file/d/1EiCGYDhztVLBPmpS2_SsFgfrWPDtsDsk/view?usp=sharing) shared for this assignment here."
      ],
      "metadata": {
        "id": "D6dAe4V0_3zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data set\n",
        "\n",
        "X = pass\n",
        "y = pass\n",
        "\n",
        "\n",
        "# Split into X_train, y_train, X_val, y_val\n",
        "# you can use stratified splitting from sklearn library\n",
        "\n"
      ],
      "metadata": {
        "id": "umr8-1EI_3ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, display one data-point in each line, with three images - the original image, the segmentation mask and the two of them superposed over each other."
      ],
      "metadata": {
        "id": "2nRhoIj9VbJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display 4 datapoints as described above \n"
      ],
      "metadata": {
        "id": "w4174DiUAUIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataset Class\n",
        "Create a Dataset class like in the previous assignment. However, this time you **should** perform augmentation.\n",
        "\n",
        "**Note -** While initializing the dataset class object, make sure you only pass the numpy arrays for images and masks. So the ```__init__``` function should look like\n",
        "```\n",
        "    def __init__(self, X, y):\n",
        "```"
      ],
      "metadata": {
        "id": "ZjY5oNGRAK1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define your dataset class\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vifSrimqBGjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ```nn.Module``` for your model\n",
        "Take inspiration from this video - https://www.youtube.com/watch?v=u1loyDCoGbE. Don't just blindly copy the code, you might not need a model as big as the original one."
      ],
      "metadata": {
        "id": "DOs6uifpBF8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a child class of nn.Module for your model\n",
        "# specify the architecture here itself\n",
        "# if necessary, make submodules in different cells for structured code\n",
        "\n"
      ],
      "metadata": {
        "id": "6Mr6_5pzGRjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "As in the previous assignments, you will write a training loop here. Learning from past experience, do maintain a validation set and monitor loss/accuracy on that to save the best model (instead of having to interrupt execution)."
      ],
      "metadata": {
        "id": "tVTyirdELXlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, train_loader, val_loader, display_step=None):\n",
        "    pass"
      ],
      "metadata": {
        "id": "z0BnrNm8LN5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize weights\n",
        "Write a small function to initialize weights for your model. You don't need to do it individually for each layer, there are ways to do it in a simple ```for``` loop."
      ],
      "metadata": {
        "id": "g319ipPXMh0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(...):\n",
        "    pass"
      ],
      "metadata": {
        "id": "GRqqKNLZMjDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction & Accuracy\n",
        "Prediction function should predict outputs using your trained model for a given **NumPy array** ```X_test``` and the output should be another **NumPy array**, the mask you predict.\n",
        "\n",
        "The accuracy function would be the number of pixels you get a correct mask for."
      ],
      "metadata": {
        "id": "ivuHRGtfN3sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, X_test):\n",
        "    pass"
      ],
      "metadata": {
        "id": "cPX1q_0AN3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(pred, labels):\n",
        "    pass"
      ],
      "metadata": {
        "id": "_nKROVpWOa6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actually training your model\n",
        "- Create a model, initialize it. Define optimizer for the model as well as loss criterion (you can actually set the seed here again, just in case you did some ```rand``` calls above for testing your functions).\n",
        "- Define an instance of the dataset class, wrap it in a dataloader.\n",
        "- Call the train function and train your model!\n"
      ],
      "metadata": {
        "id": "8aA1EWZmMbQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "F8JG_XURNLmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run your model for the validation dataset\n",
        "Use your trained model to get predictions for the validation dataset you split earlier."
      ],
      "metadata": {
        "id": "OQsiK0-COe6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i_B_NUjUOq3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This task is not like the usual classification task, you also need to check how the model is doing visually to appreciate the level of learning. Visualize 10 outputs below (as you did in the visualization cell near the top of the notebook)."
      ],
      "metadata": {
        "id": "HWTsuQejdH7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9xwXc67ldVqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "To submit your solution, you will need to make a file with name ```model.py``` containing imports necessary to write the model class and the model class itself. It shouldn't do anything else when run. Also create a file ```dataset.py``` with the dataset class and all necessary imports. Other than this, save the trained model in a file named ```ass_4.pt```. When you are done with the assignment, commit the updated notebook, the ```model.py```, ```dataset.py``` class files and the ```ass_4.pt``` model-weights file to the repository.\n",
        "\n",
        "Note that if you use submodules, all of them should go into the `model.py` file."
      ],
      "metadata": {
        "id": "0f4W_facj-PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "torch.save(final_model, 'ass_4.pt')\n",
        "files.download('ass_4.pt') # download the file from the Colab session for submission"
      ],
      "metadata": {
        "id": "7tknYAy1j92M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if it got saved right!"
      ],
      "metadata": {
        "id": "flMRBW9Akhkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model, use predict function\n"
      ],
      "metadata": {
        "id": "-wA9nHzYkj1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}