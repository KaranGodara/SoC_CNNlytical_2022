{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_lytical_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CNN-lytical Assignment-3**"
      ],
      "metadata": {
        "id": "DSGNLRqZgiJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries here**"
      ],
      "metadata": {
        "id": "XhKBj4oEgogQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn import preprocessing\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "4bVCQX6Wgxwy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "num_workers = 4"
      ],
      "metadata": {
        "id": "UQ1irherAhVm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "vNXv_G0Pg8zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into X_train, y_train, X_test, y_test\n",
        "# you can use stratified splitting from sklearn library\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "#drive already mounted\n",
        "drive.mount('/content/drive') \n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/cs231n/assignments/assignment3/train_cifar.pkl\"\n",
        "\n",
        "\n",
        "with open(DATA_PATH, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X = data['X']\n",
        "y = data['y']\n",
        "\n",
        "#split\n",
        "X = torch.reshape(torch.from_numpy(X),(50000,3,32,32))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.1) \n",
        "\n",
        "X_train = X_train.float()\n",
        "X_test = X_test.float()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCNcHaXAg7PF",
        "outputId": "02d713c5-8786-4937-f17a-a60c2d31ba12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([                                 \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n"
      ],
      "metadata": {
        "id": "nqI9l-yy-0eY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display a 4x4 grid, \n",
        "# choose 16 images randomly, display the images as well as corresponding labels\n"
      ],
      "metadata": {
        "id": "vMCYcbzrhF8K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Dataset Class**"
      ],
      "metadata": {
        "id": "fdaHHsMhhJVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define your dataset class\n",
        "\n",
        "class CIFAR_Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y, transform = transform_train):\n",
        "        'Initialization'\n",
        "        #X for images, y for labels\n",
        "        y = torch.from_numpy(y)\n",
        "        y = y.squeeze(1)\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        image = self.X[index]\n",
        "        label = self.y[index]\n",
        "        image = torch.div(image, 255.0)\n",
        "        return image, label\n",
        "\n",
        "train_dataset = CIFAR_Dataset(X_train, y_train)\n",
        "trainset_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
        "\n",
        "test_dataset = CIFAR_Dataset(X_test, y_test, transform_test)\n",
        "testset_loader = torch.utils.data.DataLoader(test_dataset, batch_size = y_test.shape[0], shuffle = False, num_workers = num_workers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-4wsOIRhM6b",
        "outputId": "dee448a9-d76d-4156-8a86-63745567f27b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nn.Module for your model**"
      ],
      "metadata": {
        "id": "NPZ9niIshO4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a child class of nn.Module for your model\n",
        "# specify the architecture here itself\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=48, kernel_size=(3,3), padding=(1,1))\n",
        "        self.conv2 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=(3,3), padding=(1,1))\n",
        "        self.conv3 = nn.Conv2d(in_channels=96, out_channels=192, kernel_size=(3,3), padding=(1,1))\n",
        "        self.conv4 = nn.Conv2d(in_channels=192, out_channels=256, kernel_size=(3,3), padding=(1,1))\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(in_features=8*8*256, out_features=512)\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=64)\n",
        "        self.Dropout = nn.Dropout(0.25)\n",
        "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x)) #32*32*48\n",
        "        x = F.relu(self.conv2(x)) #32*32*96\n",
        "        x = self.pool(x) #16*16*96\n",
        "        x = self.Dropout(x)\n",
        "        x = F.relu(self.conv3(x)) #16*16*192\n",
        "        x = F.relu(self.conv4(x)) #16*16*256\n",
        "        x = self.pool(x) # 8*8*256\n",
        "        x = self.Dropout(x)\n",
        "        x = x.view(-1, 8*8*256) # reshape x\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.Dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "o_7kBr10hTyC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction & Accuracy**"
      ],
      "metadata": {
        "id": "DlicbkYhhkg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, X_test, y_test):\n",
        "  test_dataset = CIFAR_Dataset(X_test, y_test, transform_test)\n",
        "  testset_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = num_workers)\n",
        "\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # arr = np.empty((50,), int)\n",
        "  for data in testset_loader:\n",
        "        images, labels = data\n",
        "        images = Variable(images).cuda()\n",
        "        output = model(images)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        labels = labels.numpy()\n",
        "        predicted = predicted.cpu().data.numpy()\n",
        "        # arr = np.append(arr, predicted)\n",
        "        correct += (predicted== labels).sum()\n",
        "        total += labels.size\n",
        "  # return predicted.numpy()\n",
        "  print('Accuracy: %f %%' % (100.0 * correct / float(total)))\n",
        "  # return arr"
      ],
      "metadata": {
        "id": "YmiiOjCChojs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(pred, labels):\n",
        "  correct = 0\n",
        "  correct += (pred== labels).sum()\n",
        "  total = labels.size\n",
        "  print('Accuracy: %f %%' % (100.0 * correct / float(total)))\n"
      ],
      "metadata": {
        "id": "9PPNW3IjhqXW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop**"
      ],
      "metadata": {
        "id": "kObSsovqhV9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "def train(model, criterion, trainset_loader, display_step = None):\n",
        "  #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "  for epoch in range(epochs):\n",
        "      running_loss = 0.0\n",
        "\n",
        "      if epoch <= 10:\n",
        "          optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "      elif epoch > 10 and epoch <= 25:\n",
        "          optimizer = optim.Adam(model.parameters(), lr=(learning_rate)/10)\n",
        "      else:\n",
        "          optimizer = optim.Adam(model.parameters(), lr=(learning_rate)/50)  \n",
        "\n",
        "      for i, data in enumerate(trainset_loader):\n",
        "          input_data, labels = data # data is a list of 2, the first element is 4*3*32*32 (4 images) the second element is a list of 4 (classes)\n",
        "          input_data, labels = Variable(input_data).cuda(),Variable(labels).cuda()\n",
        "\n",
        "          optimizer.zero_grad() # every time reset the parameter gradients to zero\n",
        "\n",
        "          # forward backward optimize\n",
        "          output = model(input_data)\n",
        "          loss = criterion(output, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print the loss\n",
        "          running_loss += loss.item()\n",
        "      # print the loss after every epoch\n",
        "      print('loss in epoch ' + str(epoch + 1) + ': ' + str(running_loss / y_train.size))  \n",
        "\n",
        "      #Test for accuracy after every 5 epochs  \n",
        "      if (epoch + 1)%5 == 0:\n",
        "          predict(model, X_test, y_test)\n",
        "          # accuracy(pred, y_test)\n",
        "      elif epoch == epochs - 1:\n",
        "          predict(model, X_test, y_test)\n",
        "          # accuracy(pred, y_test)\n"
      ],
      "metadata": {
        "id": "7PdhefouhbgC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize weights**"
      ],
      "metadata": {
        "id": "tkXls7OkhdcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights():\n",
        "  #On internet articles were saying pyTorch automatically initializes weights for us\n",
        "  #and those work pretty well, so have postponed the self-initialisation for now\n",
        "  pass"
      ],
      "metadata": {
        "id": "znIjo-rlhioA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actually training your model**"
      ],
      "metadata": {
        "id": "YehO0AGVhsR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet()\n",
        "model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(model, criterion, trainset_loader)\n",
        "predict(model, X_train, y_train)\n",
        "# accuracy(pred_train, y_train)\n",
        "print()\n",
        "predict(model, X_test, y_test)\n",
        "# accuracy(pred_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYzklb23h16b",
        "outputId": "4b5d338f-a6a5-47e6-eafe-44be65a0885e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 1: 0.03932684526708391\n",
            "loss in epoch 2: 0.03370031324227651\n",
            "loss in epoch 3: 0.031452283612887065\n",
            "loss in epoch 4: 0.029794972456826104\n",
            "loss in epoch 5: 0.02835657064517339\n",
            "Accuracy: 50.220000 %\n",
            "loss in epoch 6: 0.025762310294310253\n",
            "loss in epoch 7: 0.02369705669482549\n",
            "loss in epoch 8: 0.02168633870681127\n",
            "loss in epoch 9: 0.019221675834390852\n",
            "loss in epoch 10: 0.016489892703957027\n",
            "Accuracy: 52.920000 %\n",
            "loss in epoch 11: 0.0139261148194472\n",
            "loss in epoch 12: 0.007475222032268842\n",
            "loss in epoch 13: 0.0056405608953701124\n",
            "loss in epoch 14: 0.004516532034344143\n",
            "loss in epoch 15: 0.0035895001195371153\n",
            "Accuracy: 53.500000 %\n",
            "loss in epoch 16: 0.0027734548457380797\n",
            "loss in epoch 17: 0.002069966994681292\n",
            "loss in epoch 18: 0.0014939992480393913\n",
            "loss in epoch 19: 0.001020453080214146\n",
            "loss in epoch 20: 0.0006557459107393191\n",
            "Accuracy: 52.880000 %\n",
            "Accuracy: 99.775556 %\n",
            "\n",
            "Accuracy: 52.880000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submission**"
      ],
      "metadata": {
        "id": "LkKQlZd-h-Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# torch.save(model, 'ass_3.pt')\n",
        "# files.download('ass_3.pt') # download the file from the Colab session for submission"
      ],
      "metadata": {
        "id": "5AtM25G4iEF7"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}