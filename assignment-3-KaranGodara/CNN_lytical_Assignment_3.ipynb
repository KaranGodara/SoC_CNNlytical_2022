{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_lytical_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CNN-lytical Assignment-3**"
      ],
      "metadata": {
        "id": "DSGNLRqZgiJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries here**"
      ],
      "metadata": {
        "id": "XhKBj4oEgogQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn import preprocessing\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    # np.random.seed(seed)\n",
        "    # torch.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed(seed)\n",
        "    # # When running on the CuDNN backend, two further options must be set\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    # torch.backends.cudnn.benchmark = False\n",
        "    # # Set a fixed value for the hash seed\n",
        "    # os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "4bVCQX6Wgxwy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "num_workers = 4"
      ],
      "metadata": {
        "id": "UQ1irherAhVm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "vNXv_G0Pg8zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into X_train, y_train, X_test, y_test\n",
        "# you can use stratified splitting from sklearn library\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "#drive already mounted\n",
        "drive.mount('/content/drive') \n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/cs231n/assignments/assignment3/train_cifar.pkl\"\n",
        "\n",
        "\n",
        "with open(DATA_PATH, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X = data['X']\n",
        "y = data['y']\n",
        "\n",
        "#split\n",
        "X = torch.reshape(torch.from_numpy(X),(50000,3,32,32))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.1) \n",
        "\n",
        "X_train = X_train.float()\n",
        "X_test = X_test.float()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCNcHaXAg7PF",
        "outputId": "5652afa6-bc93-4d82-ca46-b6fc8c566245"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "<class 'numpy.ndarray'>\n",
            "(45000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([                                 \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqI9l-yy-0eY",
        "outputId": "f4f9059c-ba12-444f-f7b4-fbb43da33bec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([45000, 3, 32, 32])\n",
            "(45000, 1)\n",
            "torch.Size([5000, 3, 32, 32])\n",
            "(5000, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display a 4x4 grid, \n",
        "# choose 16 images randomly, display the images as well as corresponding labels\n"
      ],
      "metadata": {
        "id": "vMCYcbzrhF8K"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Dataset Class**"
      ],
      "metadata": {
        "id": "fdaHHsMhhJVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define your dataset class\n",
        "\n",
        "class CIFAR_Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y, transform = transform_train):\n",
        "        'Initialization'\n",
        "        #X for images, y for labels\n",
        "        y = torch.from_numpy(y)\n",
        "        y = y.squeeze(1)\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        image = self.X[index]\n",
        "        label = self.y[index]\n",
        "        image = torch.div(image, 255.0)\n",
        "        return image, label\n",
        "\n",
        "train_dataset = CIFAR_Dataset(X_train, y_train)\n",
        "trainset_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
        "\n",
        "test_dataset = CIFAR_Dataset(X_test, y_test, transform_test)\n",
        "testset_loader = torch.utils.data.DataLoader(test_dataset, batch_size = y_test.shape[0], shuffle = False, num_workers = num_workers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-4wsOIRhM6b",
        "outputId": "74e87838-aeb8-4deb-a5ee-6dde81378b53"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nn.Module for your model**"
      ],
      "metadata": {
        "id": "NPZ9niIshO4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a child class of nn.Module for your model\n",
        "# specify the architecture here itself\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=48, kernel_size=(3,3), padding=(1,1))\n",
        "        self.conv2 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=(3,3), padding=(1,1))\n",
        "        self.conv3 = nn.Conv2d(in_channels=96, out_channels=192, kernel_size=(3,3), padding=(1,1))\n",
        "        self.conv4 = nn.Conv2d(in_channels=192, out_channels=256, kernel_size=(3,3), padding=(1,1))\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(in_features=8*8*256, out_features=512)\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=64)\n",
        "        self.Dropout = nn.Dropout(0.25)\n",
        "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x)) #32*32*48\n",
        "        x = F.relu(self.conv2(x)) #32*32*96\n",
        "        x = self.pool(x) #16*16*96\n",
        "        x = self.Dropout(x)\n",
        "        x = F.relu(self.conv3(x)) #16*16*192\n",
        "        x = F.relu(self.conv4(x)) #16*16*256\n",
        "        x = self.pool(x) # 8*8*256\n",
        "        x = self.Dropout(x)\n",
        "        x = x.view(-1, 8*8*256) # reshape x\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.Dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "o_7kBr10hTyC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction & Accuracy**"
      ],
      "metadata": {
        "id": "DlicbkYhhkg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, X_test, y_test):\n",
        "  test_dataset = CIFAR_Dataset(X_test, y_test, transform_test)\n",
        "  testset_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = num_workers)\n",
        "\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # arr = np.empty((50,), int)\n",
        "  for data in testset_loader:\n",
        "        images, labels = data\n",
        "        images = Variable(images).cuda()\n",
        "        output = model(images)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        labels = labels.numpy()\n",
        "        predicted = predicted.cpu().data.numpy()\n",
        "        # arr = np.append(arr, predicted)\n",
        "        correct += (predicted== labels).sum()\n",
        "        total += labels.size\n",
        "  # return predicted.numpy()\n",
        "  print('Accuracy: %f %%' % (100.0 * correct / float(total)))\n",
        "  # return arr"
      ],
      "metadata": {
        "id": "YmiiOjCChojs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(pred, labels):\n",
        "  correct = 0\n",
        "  correct += (pred== labels).sum()\n",
        "  total = labels.size\n",
        "  print('Accuracy: %f %%' % (100.0 * correct / float(total)))\n"
      ],
      "metadata": {
        "id": "9PPNW3IjhqXW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop**"
      ],
      "metadata": {
        "id": "kObSsovqhV9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "def train(model, criterion, trainset_loader, display_step = None):\n",
        "  #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "  for epoch in range(epochs):\n",
        "      running_loss = 0.0\n",
        "\n",
        "      if epoch <= 10:\n",
        "          optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "      elif epoch > 10 and epoch <= 25:\n",
        "          optimizer = optim.Adam(model.parameters(), lr=(learning_rate)/10)\n",
        "      else:\n",
        "          optimizer = optim.Adam(model.parameters(), lr=(learning_rate)/50)  \n",
        "\n",
        "      for i, data in enumerate(trainset_loader):\n",
        "          input_data, labels = data # data is a list of 2, the first element is 4*3*32*32 (4 images) the second element is a list of 4 (classes)\n",
        "          input_data, labels = Variable(input_data).cuda(),Variable(labels).cuda()\n",
        "\n",
        "          optimizer.zero_grad() # every time reset the parameter gradients to zero\n",
        "\n",
        "          # forward backward optimize\n",
        "          output = model(input_data)\n",
        "          loss = criterion(output, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print the loss\n",
        "          running_loss += loss.item()\n",
        "      # print the loss after every epoch\n",
        "      print('loss in epoch ' + str(epoch + 1) + ': ' + str(running_loss / y_train.size))  \n",
        "\n",
        "      #Test for accuracy after every 5 epochs  \n",
        "      if (epoch + 1)%5 == 0:\n",
        "          predict(model, X_test, y_test)\n",
        "          # accuracy(pred, y_test)\n",
        "      elif epoch == epochs - 1:\n",
        "          predict(model, X_test, y_test)\n",
        "          # accuracy(pred, y_test)\n"
      ],
      "metadata": {
        "id": "7PdhefouhbgC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize weights**"
      ],
      "metadata": {
        "id": "tkXls7OkhdcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights():\n",
        "  #On internet articles were saying pyTorch automatically initializes weights for us\n",
        "  #and those work pretty well, so have postponed the self-initialisation for now\n",
        "  pass"
      ],
      "metadata": {
        "id": "znIjo-rlhioA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actually training your model**"
      ],
      "metadata": {
        "id": "YehO0AGVhsR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet()\n",
        "model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(model, criterion, trainset_loader)\n",
        "predict(model, X_train, y_train)\n",
        "# accuracy(pred_train, y_train)\n",
        "print()\n",
        "predict(model, X_test, y_test)\n",
        "# accuracy(pred_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "vYzklb23h16b",
        "outputId": "00971bcf-5d1f-4772-be32-48b35b9b35ee"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 1: 0.04038413704236348\n",
            "loss in epoch 2: 0.03445098439852397\n",
            "loss in epoch 3: 0.03207280953725179\n",
            "loss in epoch 4: 0.030361873586972555\n",
            "loss in epoch 5: 0.029007773133118947\n",
            "Accuracy: 50498.600000 %\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-481d48315951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-4ba33ef1a827>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, trainset_loader, display_step)\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submission**"
      ],
      "metadata": {
        "id": "LkKQlZd-h-Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# torch.save(model, 'ass_3.pt')\n",
        "# files.download('ass_3.pt') # download the file from the Colab session for submission"
      ],
      "metadata": {
        "id": "5AtM25G4iEF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}